# Hourly feature ingest + prediction (force use of RandomForest from Hopsworks)
on:
  schedule:
    - cron: '0 * * * *'   # At the top of every hour (UTC)
  workflow_dispatch:     # Allow manual runs from the Actions UI

concurrency:
  group: hourly-ingest-predict
  cancel-in-progress: true

jobs:
  ingest_and_predict:
    name: Ingest features and produce predictions (RF only)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          lfs: false
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.7'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies (including system libs for confluent-kafka)
        run: |
          sudo apt-get update -y
          sudo apt-get install -y librdkafka-dev build-essential || true
          python -m pip install --upgrade pip setuptools wheel
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install python-dotenv
        shell: bash

      - name: Prepare .env from repository secret 'AQI'
        run: |
          printf "%s\n" "${{ secrets.AQI }}" > .env
          echo "Wrote .env from secrets.AQI (values kept hidden)"
        shell: bash

      - name: Run feature pipeline (fetch & ingest)
        id: pipeline
        run: |
          set -euo pipefail
          mkdir -p logs
          echo "Running feature pipeline..."
          python run_feature_pipeline.py 2>&1 | tee logs/run_feature_pipeline.log
        shell: bash

      - name: Run predictions (force RandomForest from Hopsworks)
        id: predict
        continue-on-error: true
        env:
          # Force model selection policy: always try Hopsworks and use the RF model
          MODEL_SOURCE: "hopsworks"
          PREFERRED_MODEL: "rf"
          # Use the exact RF model name you uploaded to Hopsworks (update if different)
          HOPSWORKS_MODEL_NAME: "aqi_rf_standard_aqi_next_24h_20251030T153032Z"
          # Hopsworks / OpenWeather secrets (map from Actions secrets). Optional if .env already contains them.
          HOPSWORKS_HOST: ${{ secrets.HOPSWORKS_HOST }}
          HOPSWORKS_API_KEY: ${{ secrets.HOPSWORKS_API_KEY }}
          HOPSWORKS_PROJECT_ID: ${{ secrets.HOPSWORKS_PROJECT_ID }}
          HOPSWORKS_FEATURE_GROUP: ${{ secrets.HOPSWORKS_FEATURE_GROUP }}
          HOPSWORKS_FEATURE_GROUP_VERSION: ${{ secrets.HOPSWORKS_FEATURE_GROUP_VERSION }}
          OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
        run: |
          set -euo pipefail
          mkdir -p artifacts predictions logs
          echo "Running prediction script (MODEL_SOURCE=$MODEL_SOURCE, PREFERRED_MODEL=$PREFERRED_MODEL, HOPSWORKS_MODEL_NAME=$HOPSWORKS_MODEL_NAME)..."
          # Run predict.py; keep output in logs and move produced predictions.csv to artifacts
          python predict.py 2>&1 | tee logs/predict_output.txt || true
          if [ -f predictions.csv ]; then mv -f predictions.csv artifacts/ || true; fi
          # Debug: print which RF-related files exist locally and show head of logs
          echo "Local models/ listing (for debug):"
          ls -la models || true
          echo "Predict logs head:"
          head -n 200 logs/predict_output.txt || true
        shell: bash

      - name: Upload logs & artifacts
        uses: actions/upload-artifact@v4
        with:
          name: hourly-ingest-predict-artifacts
          path: |
            logs
            artifacts
            predictions || true
