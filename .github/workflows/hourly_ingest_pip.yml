# Hourly feature ingest + prediction -> keeps feature store & predictions fresh for dashboard
on:
  schedule:
    - cron: '0 * * * *'   # At the top of every hour (UTC)
  workflow_dispatch:     # Allow manual runs from the Actions UI

concurrency:
  group: hourly-ingest-predict
  cancel-in-progress: true

jobs:
  ingest_and_predict:
    name: Ingest features and produce predictions
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          lfs: true
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirement.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies (including system libs for confluent-kafka)
        run: |
          sudo apt-get update -y
          # Install librdkafka and build tools for confluent-kafka if required
          sudo apt-get install -y librdkafka-dev build-essential || true
          python -m pip install --upgrade pip setuptools wheel
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          # ensure python-dotenv available even if requirements install skipped for some reason
          pip install python-dotenv
        shell: bash

      - name: Prepare .env from repository secret 'AQI'
        # NOTE: this step assumes you have stored the entire multi-line .env content in the repository secret named 'AQI' (case-sensitive).
        # If 'AQI' only contains a single value (e.g. only the API key), replace this step with the mapping variant and tell me and I'll provide it.
        run: |
          printf "%s\n" "${{ secrets.AQI }}" > .env
          echo "Wrote .env from secrets.AQI (kept values hidden)"
        shell: bash

      - name: Run feature pipeline (fetch & ingest)
        id: pipeline
        run: |
          set -euo pipefail
          mkdir -p logs
          echo "Running feature pipeline..."
          python run_feature_pipeline.py 2>&1 | tee logs/run_feature_pipeline.log
        shell: bash

      - name: Run predictions
        id: predict
        continue-on-error: true
        run: |
          set -euo pipefail
          mkdir -p artifacts predictions logs
          echo "Running prediction script..."
          python predict.py 2>&1 | tee logs/predict_output.txt || true
          if [ -f predictions.csv ]; then mv -f predictions.csv artifacts/ || true; fi
        shell: bash

      - name: Upload logs & artifacts
        uses: actions/upload-artifact@v4
        with:
          name: hourly-ingest-predict-artifacts
          path: |
            logs
            artifacts
            predictions || true
