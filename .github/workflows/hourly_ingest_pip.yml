# Hourly feature ingest + prediction (updated to prefer RandomForest from Hopsworks)
on:
  schedule:
    - cron: '0 * * * *'   # At the top of every hour (UTC)
  workflow_dispatch:     # Allow manual runs from the Actions UI

concurrency:
  group: hourly-ingest-predict
  cancel-in-progress: true

jobs:
  ingest_and_predict:
    name: Ingest features and produce predictions
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          lfs: true
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies (including system libs for confluent-kafka)
        run: |
          sudo apt-get update -y
          # Install librdkafka and build tools for confluent-kafka if required
          sudo apt-get install -y librdkafka-dev build-essential || true
          python -m pip install --upgrade pip setuptools wheel
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          # ensure python-dotenv available even if requirements install skipped for some reason
          pip install python-dotenv
        shell: bash

      - name: Prepare .env from repository secret 'AQI'
        # NOTE: this step assumes you have stored the entire multi-line .env content in the repository secret named 'AQI' (case-sensitive).
        run: |
          printf "%s\n" "${{ secrets.AQI }}" > .env
          echo "Wrote .env from secrets.AQI (kept values hidden)"
        shell: bash

      - name: Run feature pipeline (fetch & ingest)
        id: pipeline
        run: |
          set -euo pipefail
          mkdir -p logs
          echo "Running feature pipeline..."
          python run_feature_pipeline.py 2>&1 | tee logs/run_feature_pipeline.log
        shell: bash

      - name: Run predictions (use RandomForest from Hopsworks)
        id: predict
        continue-on-error: true
        env:
          # Force model selection policy: prefer Hopsworks artifacts, prefer RF model
          MODEL_SOURCE: "hopsworks"
          PREFERRED_MODEL: "rf"
          # Map Hopsworks/OpenWeather secrets if you have them separately in Actions secrets.
          # If your .env (secret 'AQI') already contains these values this is optional.
          HOPSWORKS_HOST: ${{ secrets.HOPSWORKS_HOST }}
          HOPSWORKS_API_KEY: ${{ secrets.HOPSWORKS_API_KEY }}
          HOPSWORKS_PROJECT_ID: ${{ secrets.HOPSWORKS_PROJECT_ID }}
          HOPSWORKS_FEATURE_GROUP: ${{ secrets.HOPSWORKS_FEATURE_GROUP }}
          HOPSWORKS_FEATURE_GROUP_VERSION: ${{ secrets.HOPSWORKS_FEATURE_GROUP_VERSION }}
          OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
        run: |
          set -euo pipefail
          mkdir -p artifacts predictions logs
          echo "Running prediction script (MODEL_SOURCE=$MODEL_SOURCE, PREFERRED_MODEL=$PREFERRED_MODEL)..."
          # run predict.py; keep output in logs and move produced predictions.csv to artifacts
          python predict.py 2>&1 | tee logs/predict_output.txt || true
          if [ -f predictions.csv ]; then mv -f predictions.csv artifacts/ || true; fi
          # Report which model files exist on runner (debug)
          echo "Local models/ listing (for debug):"
          ls -la models || true
          echo "Predict logs head:"
          head -n 200 logs/predict_output.txt || true
        shell: bash

      - name: Upload logs & artifacts
        uses: actions/upload-artifact@v4
        with:
          name: hourly-ingest-predict-artifacts
          path: |
            logs
            artifacts
            predictions || true
